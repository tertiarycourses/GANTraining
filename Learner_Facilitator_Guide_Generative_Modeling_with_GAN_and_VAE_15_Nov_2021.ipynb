{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Learner/Facilitator Guide - Generative Modeling with GAN and VAE - 15 Nov 2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yhXTfKFGIrRg",
        "xk-jaN8KzRe4",
        "elFZ3rl6zaQQ",
        "_eOinCKDzidQ",
        "TPXHyrl8gtSt",
        "VksDOh2Wxee3",
        "VqbYXhyNhcon",
        "zbKvVscewtHs",
        "mBsFTAqg0v6F",
        "AgUGNhEV0uJR",
        "E2_fYgR7xvb1",
        "XVEXgKLrHd7B",
        "OFpG21tOIHqr",
        "6030CsvAQ0CO",
        "fBMbOunTIcJu",
        "kC0Xg0hverQ6",
        "Ul1fw3kierQ7",
        "pAKEcqCMyp3K",
        "j-tRA_iA09Zy",
        "3QmTAOO61CZm",
        "aoRap5AG1bpO",
        "d533H1HT1rnz",
        "0ZPGuKK-17QR",
        "RC5IFkub5i1j",
        "Qyu2REDvTYWO",
        "PTnISE7RbRyW",
        "Yb-yEaQQAUbU",
        "_IdQDeKsBBty",
        "oJtf0TB4Oa4h",
        "4Eauvhtkl6g5",
        "iY8XTIX7cpvC",
        "Xr-N8-MbW9Vo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woj6OHV5IjbB"
      },
      "source": [
        "#  Topic 1 Overview Generative Adversarial Network (GAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhXTfKFGIrRg"
      },
      "source": [
        "## A Simple GAN (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M6As1IKHgcd"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-jaN8KzRe4"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNkZLsvezVVC"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP5knjhRH-tc"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_dim):\n",
        "        super().__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Linear(img_dim, 128),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elFZ3rl6zaQQ"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6_ETB-zhEq"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGdYLAFMJNEn"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, z_dim, img_dim):\n",
        "        super().__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.Linear(z_dim, 256),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Linear(256, img_dim),\n",
        "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eOinCKDzidQ"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VimN8qtJfhB"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 3e-4\n",
        "z_dim = 64\n",
        "image_dim = 28 * 28 * 1  # 784\n",
        "batch_size = 32\n",
        "num_epochs = 50\n",
        "\n",
        "disc = Discriminator(image_dim).to(device)\n",
        "gen = Generator(z_dim, image_dim).to(device)\n",
        "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
        "transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
        ")\n",
        "\n",
        "dataset = datasets.(root=\"dataset/\", transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "writer_fake = SummaryWriter(f\"logs/fcgan/fake\")\n",
        "writer_real = SummaryWriter(f\"logs/fcgan/real\")\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPXHyrl8gtSt"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjFp04VoYfdX"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAJBARR9YhrO"
      },
      "source": [
        "%tensorboard --logdir logs/fcgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEdRK8fIzrAm"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX9CCR0CzwJV"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03K3ZdtMKwPD"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (real, _) in enumerate(loader):\n",
        "        real = real.view(-1, 784).to(device)\n",
        "        batch_size = real.shape[0]\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        noise = torch.randn(batch_size, z_dim).to(device)\n",
        "        fake = gen(noise)\n",
        "        disc_real = disc(real).view(-1)\n",
        "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake).view(-1)\n",
        "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        lossD = (lossD_real + lossD_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        lossD.backward(retain_graph=True)\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        # where the second option of maximizing doesn't suffer from\n",
        "        # saturating gradients\n",
        "        output = disc(fake).view(-1)\n",
        "        lossG = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        lossG.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
        "                data = real.reshape(-1, 1, 28, 28)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
        "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
        "\n",
        "                writer_fake.add_image(\n",
        "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
        "                )\n",
        "                writer_real.add_image(\n",
        "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
        "                )\n",
        "                step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VksDOh2Wxee3"
      },
      "source": [
        "## A Simple GAN (Tensorflow)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFAmi-nKxdUg"
      },
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aypyblZrzAIV"
      },
      "source": [
        "### Load and prepare the dataset\n",
        "You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQVUujguxrM0"
      },
      "source": [
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsWEmhZFx9SZ"
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsrssDE4x_eP"
      },
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgl3eKgTyDlV"
      },
      "source": [
        "# Batch and shuffle the data\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQUT5JpdzFQB"
      },
      "source": [
        "### Create the models¶\n",
        "Both the generator and discriminator are defined using the Keras Sequential API.\n",
        "\n",
        "#### The Generator\n",
        "The generator uses tf.keras.layers.Conv2DTranspose (upsampling) layers to produce an image from a seed (random noise). Start with a Dense layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the tf.keras.layers.LeakyReLU activation for each layer, except the output layer which uses tanh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1t7DvdmyHw1"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5cSRx46yKd_"
      },
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJygxD7EzLkD"
      },
      "source": [
        "### The Discriminator\n",
        "The discriminator is a CNN-based image classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RPUYqCCyORX"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
        "                                     input_shape=[28, 28, 1]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVpyx-qOzP4j"
      },
      "source": [
        "Use the (as yet untrained) discriminator to classify the generated images as real or fake. The model will be trained to output positive values for real images, and negative values for fake images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWdAtotRyR9M"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6HonMKbzT-S"
      },
      "source": [
        "### Define the loss and optimizers\n",
        "Define loss functions and optimizers for both models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x14TQaF-yVf5"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jS2cQxVzXTa"
      },
      "source": [
        "### Discriminator loss\n",
        "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j7tJW7OyYG-"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8s-gwUALzbQU"
      },
      "source": [
        "### Generator loss\n",
        "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79vG-e0QycDS"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvB7VvJyyenD"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m50ioFPhzfAV"
      },
      "source": [
        "### Define the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrvimcmQyhFI"
      },
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DTobpB8zkLj"
      },
      "source": [
        "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkfW86KzykKF"
      },
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1WFWlYqymmQ"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVMQhWnqznNw"
      },
      "source": [
        "Generate and save images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUQrVt33yo5s"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Ml6EPkzqbg"
      },
      "source": [
        "### Train the model\n",
        "Call the train() method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate).\n",
        "\n",
        "At the beginning of the training, the generated images look like random noise. As training progresses, the generated digits will look increasingly real. After about 50 epochs, they resemble MNIST digits. This may take about one minute / epoch with the default settings on Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei8wchQsyt32"
      },
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmANWnFmyxN-"
      },
      "source": [
        "# Display a single image using the epoch number\n",
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7uDPEZ2yzWM"
      },
      "source": [
        "display_image(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXB-Z0LIo_KU"
      },
      "source": [
        "#  Topic 2 Key GAN Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqbYXhyNhcon"
      },
      "source": [
        "## DCGAN (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdXpJAqgUyJH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt0M5U2Yz1yD"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2q982bzz4ph"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfyRtqEzhej5"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d, features_d*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features_d*2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d*2, features_d*4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features_d*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d*4, features_d*8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features_d*8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d*8, 1, 4, 1, 0),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4FmQz7Qz-sx"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_RCRvyE0Bsf"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rlzYEePhk2S"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.ConvTranspose2d(channels_noise, features_g * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(features_g* 8),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features_g * 8, features_g * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features_g * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features_g * 4, features_g * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features_g * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features_g * 2, features_g, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(features_g),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features_g, channels_img, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iil8mu3arjww"
      },
      "source": [
        "### Initialize Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkjFu00triDi"
      },
      "source": [
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiWTjDG00Dh7"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8FSX_vQh2zc"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "NOISE_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# If you train on MNIST, remember to set channels_img to 1\n",
        "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,download=True)\n",
        "\n",
        "# comment mnist above and uncomment below if train on CelebA\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/dcgan3/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/dcgan3/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "disc.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rph_5dkt001n"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_g5TxS6wI-X"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfYe19NewSZt"
      },
      "source": [
        "%tensorboard --logdir logs/dcgan3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvB5O5HO0MaI"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q86kx5Hu0bUN"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHtPhEDfifgz"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        disc_real = disc(real).reshape(-1)\n",
        "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake.detach()).reshape(-1)\n",
        "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbKvVscewtHs"
      },
      "source": [
        "## Activity: DCGAN (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbBitd9aUzv8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRAhyErK0f9A"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPMqM4TDxHCZ"
      },
      "source": [
        "\"\"\"\n",
        "Discriminator and Generator implementation from DCGAN paper\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            #nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH415gG70j9R"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN-urLVlxHxf"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            #nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    noise_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Discriminator(in_channels, 8)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(noise_dim, in_channels, 8)\n",
        "    z = torch.randn((N, noise_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kde7Y3Tx0naF"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2bxOuH_xS6J"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "import torchvision.transforms as transforms\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "LEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\n",
        "BATCH_SIZE = 128\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "NOISE_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_DISC = 64\n",
        "FEATURES_GEN = 64\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# If you train on MNIST, remember to set channels_img to 1\n",
        "# dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms,download=True)\n",
        "dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/dataset/anime_faces\", transform=transforms)\n",
        "\n",
        "# comment mnist above and uncomment below if train on CelebA\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(NOISE_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "fixed_noise = torch.randn(32, NOISE_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/dcgan_anime/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/dcgan_anime/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "disc.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBsFTAqg0v6F"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ7D14cSxad9"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIupQ9HPxdEV"
      },
      "source": [
        "%tensorboard --logdir logs/dcgan_anime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgUGNhEV0uJR"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLTvfSKyxW_0"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        disc_real = disc(real).reshape(-1)\n",
        "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake.detach()).reshape(-1)\n",
        "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2_fYgR7xvb1"
      },
      "source": [
        "## DCGAN (Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlFEzlIHYgYq"
      },
      "source": [
        "### Getting our hands on the CelebA dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXUH9Dv2YjKj"
      },
      "source": [
        "**Getting the CelebA data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkXqaJS2xy87"
      },
      "source": [
        "!mkdir celeba_gan\n",
        "!gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O celeba_gan/data.zip\n",
        "!unzip -qq celeba_gan/data.zip -d celeba_gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEryydEYlmH"
      },
      "source": [
        "**Creating a dataset from a directory of images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSbkSCrWROjs"
      },
      "source": [
        "from tensorflow import keras\n",
        "dataset = keras.utils_dataset_from_directory(\n",
        "    \"celeba_gan\",\n",
        "    label_mode=None,\n",
        "    image_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    smart_resize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnYWiz-Yn-B"
      },
      "source": [
        "**Rescaling the images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjEUCOfpYF9J"
      },
      "source": [
        "dataset = dataset.map(lambda x: x / 255.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHAix7DxYIGG"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE-L_MxNYqJz"
      },
      "source": [
        "**Displaying the first image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMB4PVVGYr9B"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Eb6kgNYuv1"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO_t4U40Yxd6"
      },
      "source": [
        "**The GAN discriminator network**\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkpHlrRfY0PW"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3EjghYMY9Ao"
      },
      "source": [
        "discriminator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_GcTdG5Y-zs"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtSwWfURZAmT"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkt8o_SeZNRU"
      },
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "re7jpfHBZPoX"
      },
      "source": [
        "generator.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5f9f5CuZR3p"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZHZSZcBZUgP"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jfV8cakZT5y"
      },
      "source": [
        "import tensorflow as tf\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(\n",
        "            shape=(batch_size, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n",
        "            axis=0\n",
        "        )\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(\n",
        "            shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(\n",
        "                self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\"d_loss\": self.d_loss_metric.result(),\n",
        "                \"g_loss\": self.g_loss_metric.result()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G2fx4ZXZtif"
      },
      "source": [
        "**A callback that samples generated images during training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgjHtr8wZv1g"
      },
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.utils.array_to_img(generated_images[i])\n",
        "            img.save(f\"generated_img_{epoch:03d}_{i}.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp6ae-30ZzCE"
      },
      "source": [
        "**Compiling and training the GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GktQMtrdZ05i"
      },
      "source": [
        "epochs = 100\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVEXgKLrHd7B"
      },
      "source": [
        "## WGAN (Pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdyRTo8oIBRI"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6tpSnjXIEGX"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSh_hXt6H02c"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5x6mQgeHf7Y"
      },
      "source": [
        "\"\"\"\n",
        "Discriminator and Generator implementation from DCGAN paper,\n",
        "with removed Sigmoid() as output from Discriminator (and therefor\n",
        "it should be called critic)\n",
        "\"\"\"\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFpG21tOIHqr"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeY8--coILBM"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCX714b-HsvA"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    noise_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Discriminator(in_channels, 8)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(noise_dim, in_channels, 8)\n",
        "    z = torch.randn((N, noise_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
        "\n",
        "\n",
        "# test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA4D8wBuIWns"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caZpUl1_H7-o"
      },
      "source": [
        "# Hyperparameters etc\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "Z_DIM = 128\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_CRITIC = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "WEIGHT_CLIP = 0.01\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
        "#comment mnist and uncomment below if you want to train on CelebA dataset\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# initialize gen and disc/critic\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/wgan/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/wgan/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6030CsvAQ0CO"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBFvV2DxQQC3"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl4ogCDEQykM"
      },
      "source": [
        "%tensorboard --logdir logs/wgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBMbOunTIcJu"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hf_Q85w9Ighv"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtN_5f5nH-tD"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (data, _) in enumerate(loader):\n",
        "        data = data.to(device)\n",
        "        cur_batch_size = data.shape[0]\n",
        "\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            fake = gen(noise)\n",
        "            critic_real = critic(data).reshape(-1)\n",
        "            critic_fake = critic(fake).reshape(-1)\n",
        "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "            # clip critic weights between -0.01, 0.01\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            gen.eval()\n",
        "            critic.eval()\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    data[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1\n",
        "            gen.train()\n",
        "            critic.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC0Xg0hverQ6"
      },
      "source": [
        "## Activity: WGAN (Pytorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT93oPS3erQ6"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxtCfg80erQ6"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YVXBtkWerQ7"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGUbkalYerQ7"
      },
      "source": [
        "\"\"\"\n",
        "Discriminator and Generator implementation from DCGAN paper,\n",
        "with removed Sigmoid() as output from Discriminator (and therefor\n",
        "it should be called critic)\n",
        "\"\"\"\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcZbJegterQ7"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj0kx9UoerQ7"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYn0qYNGerQ7"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "\n",
        "def test():\n",
        "    N, in_channels, H, W = 8, 3, 64, 64\n",
        "    noise_dim = 100\n",
        "    x = torch.randn((N, in_channels, H, W))\n",
        "    disc = Discriminator(in_channels, 8)\n",
        "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
        "    gen = Generator(noise_dim, in_channels, 8)\n",
        "    z = torch.randn((N, noise_dim, 1, 1))\n",
        "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
        "\n",
        "\n",
        "# test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1fw3kierQ7"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKzNw2bHerQ7"
      },
      "source": [
        "# Hyperparameters etc\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 3\n",
        "Z_DIM = 128\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_CRITIC = 64\n",
        "FEATURES_GEN = 64\n",
        "CRITIC_ITERATIONS = 5\n",
        "WEIGHT_CLIP = 0.01\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "#dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
        "#comment mnist and uncomment below if you want to train on CelebA dataset\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "\n",
        "dataset = datasets.ImageFolder(root=\"/content/drive/MyDrive/dataset/anime_faces\", transform=transforms)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# initialize gen and disc/critic\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
        "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/wgan/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/wgan/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P7oI3_serQ7"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOa20HLoerQ7"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQdJJVJBerQ7"
      },
      "source": [
        "%tensorboard --logdir logs/wgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXCMpuMmerQ8"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpZyh5QOerQ8"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "571pN5FRerQ8"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (data, _) in enumerate(loader):\n",
        "        data = data.to(device)\n",
        "        cur_batch_size = data.shape[0]\n",
        "\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            fake = gen(noise)\n",
        "            critic_real = critic(data).reshape(-1)\n",
        "            critic_fake = critic(fake).reshape(-1)\n",
        "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "            # clip critic weights between -0.01, 0.01\n",
        "            for p in critic.parameters():\n",
        "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            gen.eval()\n",
        "            critic.eval()\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    data[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1\n",
        "            gen.train()\n",
        "            critic.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAKEcqCMyp3K"
      },
      "source": [
        "## WGAN-GP (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FMacS2_y7Z8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-tRA_iA09Zy"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwZy5VZ11BN0"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biIZtGa5ysUu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
        "            ),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QmTAOO61CZm"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqrHkhAW1Iul"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S663i0FSy82j"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09gLcr5YVAkg"
      },
      "source": [
        "### Initialize Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHhrKfxdU-Hg"
      },
      "source": [
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoRap5AG1bpO"
      },
      "source": [
        "### Gradient Penalty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzXFRA0q1OxP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def gradient_penalty(critic, real, fake, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"celeba_wgan_gp.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, gen, disc):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    gen.load_state_dict(checkpoint['gen'])\n",
        "    disc.load_state_dict(checkpoint['disc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d533H1HT1rnz"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgvLgYGi1ndU"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 5\n",
        "FEATURES_CRITIC = 16\n",
        "FEATURES_GEN = 16\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
        "# comment mnist above and uncomment below for training on CelebA\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# initialize gen and disc, note: discriminator should be called critic,\n",
        "# according to WGAN paper (since it no longer outputs between [0, 1])\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/wgangp/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/wgangp/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZPGuKK-17QR"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlwfSXvk18Z6"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-siZjgm3oCa"
      },
      "source": [
        "%tensorboard --logdir logs/wgangp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGOT-W-01z4p"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MDx_2Yl148k"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ1Obakr1jDz"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(loader):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        # equivalent to minimizing the negative of that\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            fake = gen(noise)\n",
        "            critic_real = critic(real).reshape(-1)\n",
        "            critic_fake = critic(fake).reshape(-1)\n",
        "            gp = gradient_penalty(critic, real, fake, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kx00UsVefDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNTOtcJXUcZt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnF1vHXLfgHG"
      },
      "source": [
        "## Conditional GAN (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3B_AtyyfgHO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ahkKYKffgHO"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlX8ga0JfgHO"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zEc4XR2fgHO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d,num_classes,img_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.disc = nn.Sequential(\n",
        "            nn.Conv2d(channels_img+1,features_d,kernel_size=4,stride=2,padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d,features_d*2,4,2,1,bias=False),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d*2,features_d*4,4,2,1,bias=False),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d*4,features_d*8,4,2,1,bias=False),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(features_d*8,1,kernel_size=4,stride=2,padding=0),\n",
        "        )\n",
        "        self.embed = nn.Embedding(num_classes,img_size*img_size)\n",
        "\n",
        "    def forward(self, x,labels):\n",
        "        embedding = self.embed(labels).view(labels.shape[0],1,self.img_size, self.img_size)\n",
        "        x = torch.cat([x,embedding],dim=1)\n",
        "        return self.disc(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nxHgDA9fgHO"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNCMX16cfgHO"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiiWFbukfgHP"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g,num_classes,img_size,embed_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.net = nn.Sequential(\n",
        "            self._block(channels_noise+embed_size, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g*16, features_g*8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g*8, features_g*4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g*4, features_g*2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "        self.embed = nn.Embedding(num_classes,embed_size)\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x,labels):\n",
        "        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n",
        "        x = torch.cat([x,embedding],dim=1)\n",
        "        return self.net(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zQ1eKLf7cys"
      },
      "source": [
        "### Initialize Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdj84ESC7cEW"
      },
      "source": [
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKNlj7_pfgHP"
      },
      "source": [
        "### Gradient Penalty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atoH-rUZfgHP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def gradient_penalty(critic, labels, real, fake, device=\"cpu\"):\n",
        "    BATCH_SIZE, C, H, W = real.shape\n",
        "    alpha = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
        "    interpolated_images = real * alpha + fake * (1 - alpha)\n",
        "\n",
        "    # Calculate critic scores\n",
        "    mixed_scores = critic(interpolated_images,labels)\n",
        "\n",
        "    # Take the gradient of the scores with respect to the images\n",
        "    gradient = torch.autograd.grad(\n",
        "        inputs=interpolated_images,\n",
        "        outputs=mixed_scores,\n",
        "        grad_outputs=torch.ones_like(mixed_scores),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "    )[0]\n",
        "    gradient = gradient.view(gradient.shape[0], -1)\n",
        "    gradient_norm = gradient.norm(2, dim=1)\n",
        "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
        "    return gradient_penalty\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"celeba_wgan_gp.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, gen, disc):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    gen.load_state_dict(checkpoint['gen'])\n",
        "    disc.load_state_dict(checkpoint['disc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz04I8yrfgHP"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu-okwOofgHP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Hyperparameters etc.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 64\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS_IMG = 1\n",
        "NUM_CLASSES = 10\n",
        "GEN_EMBEDDING = 100\n",
        "Z_DIM = 100\n",
        "NUM_EPOCHS = 100\n",
        "FEATURES_CRITIC = 16\n",
        "FEATURES_GEN = 16\n",
        "CRITIC_ITERATIONS = 5\n",
        "LAMBDA_GP = 10\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(IMAGE_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
        "# comment mnist above and uncomment below for training on CelebA\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "# initialize gen and disc, note: discriminator should be called critic,\n",
        "# according to WGAN paper (since it no longer outputs between [0, 1])\n",
        "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN,NUM_CLASSES,IMAGE_SIZE,GEN_EMBEDDING).to(device)\n",
        "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC,NUM_CLASSES,IMAGE_SIZE).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(critic)\n",
        "\n",
        "# initializate optimizer\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "opt_critic = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
        "\n",
        "# for tensorboard plotting\n",
        "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
        "writer_real = SummaryWriter(f\"logs/cgan/real\")\n",
        "writer_fake = SummaryWriter(f\"logs/cgan/fake\")\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "critic.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s76d32n5fgHP"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2HHOAINfgHP"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAeVXl_ifgHQ"
      },
      "source": [
        "%tensorboard --logdir logs/cgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waCbYoo-fgHQ"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6xPqIDdfgHQ"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h387VYCpfgHQ"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    for batch_idx, (real, labels) in enumerate(loader):\n",
        "        real = real.to(device)\n",
        "        cur_batch_size = real.shape[0]\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
        "        # equivalent to minimizing the negative of that\n",
        "        for _ in range(CRITIC_ITERATIONS):\n",
        "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
        "            fake = gen(noise,labels)\n",
        "            critic_real = critic(real,labels).reshape(-1)\n",
        "            critic_fake = critic(fake,labels).reshape(-1)\n",
        "            gp = gradient_penalty(critic, labels, real, fake, device=device)\n",
        "            loss_critic = (\n",
        "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp\n",
        "            )\n",
        "            critic.zero_grad()\n",
        "            loss_critic.backward(retain_graph=True)\n",
        "            opt_critic.step()\n",
        "\n",
        "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
        "        gen_fake = critic(fake,labels).reshape(-1)\n",
        "        loss_gen = -torch.mean(gen_fake)\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
        "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(noise,labels)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
        "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8BL-tn_fgHQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CX7oByUUdS5"
      },
      "source": [
        "## Activity: Conditional GAN (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgPj1NRhUgXt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbWDuh6D5UFK"
      },
      "source": [
        "#Topic 3 Variational Autoencoders (VAEs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC5IFkub5i1j"
      },
      "source": [
        "## Autoencoder (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85XqcvV65d8C"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnZuJjtb5Yv"
      },
      "source": [
        "### The Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJHLKqQa67hA"
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "    def __init__(self,latent_dim):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(True), \n",
        "            nn.Linear(64, latent_dim))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(True), \n",
        "            nn.Linear(128, 28 * 28), \n",
        "            nn.Tanh())\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIhjoAY3cG_z"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf-NvzDR5zIg"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "import torchvision.transforms as transforms\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 3e-4\n",
        "latent_dim = 64\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "model = autoencoder(latent_dim).cuda()\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
        ")\n",
        "\n",
        "dataset = datasets.MNIST(root=\"dataset/\", train=True, transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "writer_real = SummaryWriter(f\"logs/autoenc/original\")\n",
        "writer_fake = SummaryWriter(f\"logs/autoenc/reconstructed\")\n",
        "step = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6yeNiT3cDN7"
      },
      "source": [
        "### Train the AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcezbnS77Ix"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (img, _) in enumerate(loader):\n",
        "        img = img.view(-1, 784).to(device)\n",
        "        output = model(img)\n",
        "        loss = criterion(output, img)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "                     Loss: {loss:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # take out (up to) 32 examples\n",
        "                img = img.reshape(-1, 1, 28, 28)\n",
        "                output = output.reshape(-1, 1, 28, 28)\n",
        "\n",
        "                img_grid_original = torchvision.utils.make_grid(\n",
        "                    img[:32], normalize=True\n",
        "                )\n",
        "                img_grid_output = torchvision.utils.make_grid(\n",
        "                    output[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Original Image\", img_grid_original, global_step=step)\n",
        "                writer_fake.add_image(\"Reconstructed Image\", img_grid_output, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYPozX4d7VAG"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtEQ_lM6DFu2"
      },
      "source": [
        "%tensorboard --logdir logs/autoenc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyu2REDvTYWO"
      },
      "source": [
        "## Autoencoder (Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQn9Sucb_oS"
      },
      "source": [
        "### The Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZOUGzsHDOhE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhX7MXtwTkJ_"
      },
      "source": [
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shIQmbX61eNU"
      },
      "source": [
        "latent_dim = 64 \n",
        "\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim   \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(128,activation='relu'),\n",
        "      layers.Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(128,activation='relu')    ,                              \n",
        "      layers.Dense(784, activation='sigmoid'),\n",
        "      layers.Reshape((28, 28))\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = Autoencoder(latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azx2-NNYcKyt"
      },
      "source": [
        "### Train the Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-OOk9QET3Nn"
      },
      "source": [
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w3uOCn0Tpy3"
      },
      "source": [
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test)))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dRiuRndTupl"
      },
      "source": [
        "# encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "# decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()\n",
        "decoded_imgs = autoencoder(x_test).numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8EAUHhB1lo2"
      },
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(\"reconstructed\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTnISE7RbRyW"
      },
      "source": [
        "## Variational Autoencoder VAE (Pytorch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzczg1U2ba97"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omNhJkZ-jcvL"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wE5hCkuhzoy"
      },
      "source": [
        "class Encoder(nn.Module):    \n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        self.training = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x      = self.LeakyReLU(self.FC_input(x))\n",
        "        x       = self.LeakyReLU(self.FC_input2(x))\n",
        "        mean     = self.FC_mean(x)\n",
        "        log_var  = self.FC_var(x)                   \n",
        "                                                    \n",
        "        return mean, log_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID9IvDmRkIZW"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf2K6Mt2h2gP"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x     = self.LeakyReLU(self.FC_hidden(x))\n",
        "        x     = self.LeakyReLU(self.FC_hidden2(x))\n",
        "        x_hat = torch.sigmoid(self.FC_output(x))\n",
        "        return x_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTNKj7JlkbwC"
      },
      "source": [
        "### The VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Lm40Bih4KV"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "        \n",
        "    def reparameterization(self, mean, var):\n",
        "        epsilon = torch.randn_like(var).to(device)            \n",
        "        z = mean + var*epsilon               \n",
        "        return z\n",
        "                \n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.Encoder(x)\n",
        "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) \n",
        "        x_hat = self.Decoder(z)\n",
        "        \n",
        "        return x_hat, mean, log_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-P5e0_-Sljqu"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "import torchvision.transforms as transforms\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 3e-4\n",
        "image_dim = 28 * 28 * 1  # 784\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "x_dim  = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "epochs = 5\n",
        "\n",
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(device)\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
        ")\n",
        "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "writer_real = SummaryWriter(f\"logs/vae/original\")\n",
        "writer_fake = SummaryWriter(f\"logs/vae/output\")\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYWLCBv0h71l"
      },
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLLzmtY8h_25"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (img, _) in enumerate(loader):\n",
        "        img = img.view(batch_size, x_dim)\n",
        "        img = img.to(device)\n",
        "\n",
        "        x_hat, mean, log_var  = model(img)\n",
        "        loss = loss_function(img, x_hat, mean, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "                     Loss: {loss:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # take out (up to) 32 examples\n",
        "                img = img.reshape(-1, 1, 28, 28)\n",
        "                x_hat = x_hat.reshape(-1, 1, 28, 28)\n",
        "\n",
        "                img_grid_original = torchvision.utils.make_grid(\n",
        "                    img[:32], normalize=True\n",
        "                )\n",
        "                img_grid_output = torchvision.utils.make_grid(\n",
        "                    x_hat[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Original Image\", img_grid_original, global_step=step)\n",
        "                writer_fake.add_image(\"Output Image\", img_grid_output, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEgx9JukSDbT"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8DBZpk3pJro"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vPApk3e3vWd"
      },
      "source": [
        "%tensorboard --logdir logs/vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-yEaQQAUbU"
      },
      "source": [
        "## Activity: Variational Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8lSvVj4SjI1"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9apns-NSjI1"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO9z1jPoSjI1"
      },
      "source": [
        "class Encoder(nn.Module):    \n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.FC_input = nn.Linear(input_dim, hidden_dim)\n",
        "        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        self.training = True\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x      = self.LeakyReLU(self.FC_input(x))\n",
        "        x       = self.LeakyReLU(self.FC_input2(x))\n",
        "        mean     = self.FC_mean(x)\n",
        "        log_var  = self.FC_var(x)                   \n",
        "                                                    \n",
        "        return mean, log_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y-v70tGSjI2"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJ1B0ZrASjI2"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.FC_output = nn.Linear(hidden_dim, output_dim)\n",
        "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x     = self.LeakyReLU(self.FC_hidden(x))\n",
        "        x     = self.LeakyReLU(self.FC_hidden2(x))\n",
        "        x_hat = torch.sigmoid(self.FC_output(x))\n",
        "        return x_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OsKgKSpSjI5"
      },
      "source": [
        "### The VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhy6azoBSjI5"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, Encoder, Decoder):\n",
        "        super(Model, self).__init__()\n",
        "        self.Encoder = Encoder\n",
        "        self.Decoder = Decoder\n",
        "        \n",
        "    def reparameterization(self, mean, var):\n",
        "        epsilon = torch.randn_like(var).to(device)            \n",
        "        z = mean + var*epsilon               \n",
        "        return z\n",
        "                \n",
        "    def forward(self, x):\n",
        "        mean, log_var = self.Encoder(x)\n",
        "        z = self.reparameterization(mean, torch.exp(0.5 * log_var)) \n",
        "        x_hat = self.Decoder(z)\n",
        "        \n",
        "        return x_hat, mean, log_var"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmhYaeoiSjI5"
      },
      "source": [
        "# Hyperparameters etc.\n",
        "import torchvision.transforms as transforms\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "lr = 3e-4\n",
        "image_dim = 28 *28 * 1  # 784\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "x_dim  = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "epochs = 5\n",
        "\n",
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(device)\n",
        "\n",
        "transforms = transforms.Compose(\n",
        "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
        ")\n",
        "dataset = datasets.FashionMNIST(root=\"dataset/\", transform=transforms, download=True)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "writer_real = SummaryWriter(f\"logs/vae2/original\")\n",
        "writer_fake = SummaryWriter(f\"logs/vae2/output\")\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49VQdUIDSjI5"
      },
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "def loss_function(x, x_hat, mean, log_var):\n",
        "    reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
        "\n",
        "    return reproduction_loss + KLD\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDyX_eBlSjI5"
      },
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (img, _) in enumerate(loader):\n",
        "        img = img.view(batch_size, x_dim)\n",
        "        img = img.to(device)\n",
        "\n",
        "        x_hat, mean, log_var  = model(img)\n",
        "        loss = loss_function(img, x_hat, mean, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
        "                     Loss: {loss:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # take out (up to) 32 examples\n",
        "                img = img.reshape(-1, 1, 28, 28)\n",
        "                x_hat = x_hat.reshape(-1, 1, 28, 28)\n",
        "\n",
        "                img_grid_original = torchvision.utils.make_grid(\n",
        "                    img[:32], normalize=True\n",
        "                )\n",
        "                img_grid_output = torchvision.utils.make_grid(\n",
        "                    x_hat[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Original Image\", img_grid_original, global_step=step)\n",
        "                writer_fake.add_image(\"Output Image\", img_grid_output, global_step=step)\n",
        "\n",
        "            step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJoqe28SjI2"
      },
      "source": [
        "### Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecDO8IySjI2"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Anp5HN0DSjI4"
      },
      "source": [
        "%tensorboard --logdir logs/vae2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IdQDeKsBBty"
      },
      "source": [
        "## Variational autoencoders (Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMe34UL0BQk9"
      },
      "source": [
        "### VAE encoder network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiPXJtlUBJ6W"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGqLOfEnBN8e"
      },
      "source": [
        "encoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Z_Nhd5BWo6"
      },
      "source": [
        "### Latent-space-sampling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmeIjdmLBUVO"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Sampler(layers.Layer):\n",
        "    def call(self, z_mean, z_log_var):\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        z_size = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, z_size))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVKJXLL8BcmM"
      },
      "source": [
        "### VAE decoder network, mapping latent space points to images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dysRWVzBZhQ"
      },
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUomZdlDBfps"
      },
      "source": [
        "decoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQNbuB-gBk99"
      },
      "source": [
        "### VAE model with custom train_step()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxEvjlu5Bi-q"
      },
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sampler = Sampler()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encoder(data)\n",
        "            z = self.sampler(z_mean, z_log_var)\n",
        "            reconstruction = decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"total_loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_LZezGpBrC5"
      },
      "source": [
        "### Training the VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04384YOwBpe2"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLW_zpTxBv5V"
      },
      "source": [
        "### Sampling a grid of images from the 2D latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAuyfTZGBuHv"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 30\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "grid_x = np.linspace(-1, 1, n)\n",
        "grid_y = np.linspace(-1, 1, n)[::-1]\n",
        "\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = vae.decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[\n",
        "            i * digit_size : (i + 1) * digit_size,\n",
        "            j * digit_size : (j + 1) * digit_size,\n",
        "        ] = digit\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "start_range = digit_size // 2\n",
        "end_range = n * digit_size + start_range\n",
        "pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "sample_range_x = np.round(grid_x, 1)\n",
        "sample_range_y = np.round(grid_y, 1)\n",
        "plt.xticks(pixel_range, sample_range_x)\n",
        "plt.yticks(pixel_range, sample_range_y)\n",
        "plt.xlabel(\"z[0]\")\n",
        "plt.ylabel(\"z[1]\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(figure, cmap=\"Greys_r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOVZPdJeOWQW"
      },
      "source": [
        "# Topic 4 GAN Applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtf0TB4Oa4h"
      },
      "source": [
        "## Pic2Pic Demo (Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMpSBASWae4G"
      },
      "source": [
        "source: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/pix2pix.ipynb#scrollTo=HSSm4kfvJiqv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ucw5JpC-axoY"
      },
      "source": [
        "This tutorial demonstrates how to build and train a conditional generative adversarial network (cGAN) called pix2pix that learns a mapping from input images to output images, as described in [Image-to-image translation with conditional adversarial networks](https://arxiv.org/abs/1611.07004) by Isola et al. (2017). pix2pix is not application specific—it can be applied to a wide range of tasks, including synthesizing photos from label maps, generating colorized photos from black and white images, turning Google Maps photos into aerial images, and even transforming sketches into photos.\n",
        "\n",
        "In this example, your network will generate images of building facades using the [CMP Facade Database](http://cmp.felk.cvut.cz/~tylecr1/facade/) provided by the [Center for Machine Perception](http://cmp.felk.cvut.cz/) at the [Czech Technical University in Prague](https://www.cvut.cz/). To keep it short, you will use a [preprocessed copy]((https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/)) of this dataset created by the pix2pix authors.\n",
        "\n",
        "In the pix2pix cGAN, you condition on input images and generate corresponding output images. cGANs were first proposed in [Conditional Generative Adversarial Nets](https://arxiv.org/abs/1411.1784) (Mirza and Osindero, 2014)\n",
        "\n",
        "The architecture of your network will contain:\n",
        "\n",
        "- A generator with a [U-Net]([U-Net](https://arxiv.org/abs/1505.04597))-based architecture.\n",
        "- A discriminator represented by a convolutional PatchGAN classifier (proposed in the [pix2pix paper](https://arxiv.org/abs/1611.07004)).\n",
        "\n",
        "Note that each epoch can take around 15 seconds on a single V100 GPU.\n",
        "\n",
        "Below are some examples of the output generated by the pix2pix cGAN after training for 200 epochs on the facades dataset (80k steps).\n",
        "\n",
        "![sample output_1](https://www.tensorflow.org/images/gan/pix2pix_1.png)\n",
        "![sample output_2](https://www.tensorflow.org/images/gan/pix2pix_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNitfpjzPBDY"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSaVY1AOaN0"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uLhHanAcSO_"
      },
      "source": [
        "### Load the dataset\n",
        "Download the CMP Facade Database data (30MB). Additional datasets are available in the same format [here](http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/). In Colab you can select other datasets from the drop-down menu. Note that some of the other datasets are significantly larger (`edges2handbags` is 8GB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyKdc3HaX9YD"
      },
      "source": [
        "dataset_name = \"facades\" #@param [\"cityscapes\", \"edges2handbags\", \"edges2shoes\", \"facades\", \"maps\", \"night2day\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhiOpjyQYBkN"
      },
      "source": [
        "_URL = f'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/{dataset_name}.tar.gz'\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    fname=f\"{dataset_name}.tar.gz\",\n",
        "    origin=_URL,\n",
        "    extract=True)\n",
        "\n",
        "path_to_zip  = pathlib.Path(path_to_zip)\n",
        "\n",
        "PATH = path_to_zip.parent/dataset_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY1ZjYYdcaOY"
      },
      "source": [
        "Each original image is of size `256 x 512` containing two `256 x 256` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bXG3QWVYLry"
      },
      "source": [
        "sample_image = tf.io.read_file(str(PATH / 'train/1.jpg'))\n",
        "sample_image = tf.io.decode_jpeg(sample_image)\n",
        "print(sample_image.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9vnJBSvYOkP"
      },
      "source": [
        "plt.figure()\n",
        "plt.imshow(sample_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9_5lQaocdvg"
      },
      "source": [
        "You need to separate real building facade images from the architecture label images—all of which will be of size `256 x 256`.\n",
        "\n",
        "Define a function that loads image files and outputs two image tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNCsvNR7YRb2"
      },
      "source": [
        "def load(image_file):\n",
        "  # Read and decode an image file to a uint8 tensor\n",
        "  image = tf.io.read_file(image_file)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "\n",
        "  # Split each image tensor into two tensors:\n",
        "  # - one with a real building facade image\n",
        "  # - one with an architecture label image \n",
        "  w = tf.shape(image)[1]\n",
        "  w = w // 2\n",
        "  input_image = image[:, w:, :]\n",
        "  real_image = image[:, :w, :]\n",
        "\n",
        "  # Convert both images to float32 tensors\n",
        "  input_image = tf.cast(input_image, tf.float32)\n",
        "  real_image = tf.cast(real_image, tf.float32)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lZxZHURcg06"
      },
      "source": [
        "Plot a sample of the input (architecture label image) and real (building facade photo) images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08g-weD9YUuV"
      },
      "source": [
        "inp, re = load(str(PATH / 'train/100.jpg'))\n",
        "# Casting to int for matplotlib to display the images\n",
        "plt.figure()\n",
        "plt.imshow(inp / 255.0)\n",
        "plt.figure()\n",
        "plt.imshow(re / 255.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Hu_KNuckRy"
      },
      "source": [
        "As described in the [pix2pix paper](https://arxiv.org/abs/1611.07004), you need to apply random jittering and mirroring to preprocess the training set.\n",
        "\n",
        "Define several functions that:\n",
        "\n",
        "1. Resize each `256 x 256` image to a larger height and width—`286 x 286`.\n",
        "2. Randomly crop it back to `256 x 256`.\n",
        "3. Randomly flip the image horizontally i.e. left to right (random mirroring).\n",
        "4. Normalize the images to the `[-1, 1]` range."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKrW6i0RYYZw"
      },
      "source": [
        "# The facade training set consist of 400 images\n",
        "BUFFER_SIZE = 400\n",
        "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
        "BATCH_SIZE = 1\n",
        "# Each image is 256x256 in size\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUt_6GrAYbM4"
      },
      "source": [
        "def resize(input_image, real_image, height, width):\n",
        "  input_image = tf.image.resize(input_image, [height, width],\n",
        "                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "  real_image = tf.image.resize(real_image, [height, width],\n",
        "                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kr09cw0YdfK"
      },
      "source": [
        "def random_crop(input_image, real_image):\n",
        "  stacked_image = tf.stack([input_image, real_image], axis=0)\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image[0], cropped_image[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udXgJ5KXYiQt"
      },
      "source": [
        "# Normalizing the images to [-1, 1]\n",
        "def normalize(input_image, real_image):\n",
        "  input_image = (input_image / 127.5) - 1\n",
        "  real_image = (real_image / 127.5) - 1\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y8-x1ChYkh3"
      },
      "source": [
        "@tf.function()\n",
        "def random_jitter(input_image, real_image):\n",
        "  # Resizing to 286x286\n",
        "  input_image, real_image = resize(input_image, real_image, 286, 286)\n",
        "\n",
        "  # Random cropping back to 256x256\n",
        "  input_image, real_image = random_crop(input_image, real_image)\n",
        "\n",
        "  if tf.random.uniform(()) > 0.5:\n",
        "    # Random mirroring\n",
        "    input_image = tf.image.flip_left_right(input_image)\n",
        "    real_image = tf.image.flip_left_right(real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IlskgBjcoLJ"
      },
      "source": [
        "You can inspect some of the preprocessed output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHDyqgVvYpJp"
      },
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "for i in range(4):\n",
        "  rj_inp, rj_re = random_jitter(inp, re)\n",
        "  plt.subplot(2, 2, i + 1)\n",
        "  plt.imshow(rj_inp / 255.0)\n",
        "  plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h5ZSDG7cvTC"
      },
      "source": [
        "Having checked that the loading and preprocessing works, let's define a couple of helper functions that load and preprocess the training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DAhAEs5YsP1"
      },
      "source": [
        "def load_image_train(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = random_jitter(input_image, real_image)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfBYuOU3YuJ6"
      },
      "source": [
        "def load_image_test(image_file):\n",
        "  input_image, real_image = load(image_file)\n",
        "  input_image, real_image = resize(input_image, real_image,\n",
        "                                   IMG_HEIGHT, IMG_WIDTH)\n",
        "  input_image, real_image = normalize(input_image, real_image)\n",
        "\n",
        "  return input_image, real_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwLyIXoPcx90"
      },
      "source": [
        "### Build an input pipeline with `tf.data`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZIUYa-AYv8Z"
      },
      "source": [
        "train_dataset = tf.data.Dataset.list_files(str(PATH / 'train/*.jpg'))\n",
        "train_dataset = train_dataset.map(load_image_train,\n",
        "                                  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEWwjORLYx9e"
      },
      "source": [
        "try:\n",
        "  test_dataset = tf.data.Dataset.list_files(str(PATH / 'test/*.jpg'))\n",
        "except tf.errors.InvalidArgumentError:\n",
        "  test_dataset = tf.data.Dataset.list_files(str(PATH / 'val/*.jpg'))\n",
        "test_dataset = test_dataset.map(load_image_test)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o59mD7ADc5eN"
      },
      "source": [
        "### Build the generator\n",
        "\n",
        "The generator of your pix2pix cGAN is a _modified_ [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). (You can find out more about it in the [Image segmentation](https://www.tensorflow.org/tutorials/images/segmentation) tutorial and on the [U-Net project website](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/).)\n",
        "\n",
        "- Each block in the encoder is: Convolution -> Batch normalization -> Leaky ReLU\n",
        "- Each block in the decoder is: Transposed convolution -> Batch normalization -> Dropout (applied to the first 3 blocks) -> ReLU\n",
        "- There are skip connections between the encoder and decoder (as in the U-Net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3zS5mR-c9V5"
      },
      "source": [
        "Define the downsampler (encoder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3IjR-TBY0Tk"
      },
      "source": [
        "OUTPUT_CHANNELS = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPk4qS8tY2mg"
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zER2gAIIY4N2"
      },
      "source": [
        "down_model = downsample(3, 4)\n",
        "down_result = down_model(tf.expand_dims(inp, 0))\n",
        "print (down_result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jWw6Gqdc_yU"
      },
      "source": [
        "Define the upsampler (decoder):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r7IblY2ZANM"
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J6niib6ZCPU"
      },
      "source": [
        "up_model = upsample(3, 4)\n",
        "up_result = up_model(down_result)\n",
        "print (up_result.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mphjkrDCdCo0"
      },
      "source": [
        "Define the generator with the downsampler and the upsampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u7nAJg1ZFsZ"
      },
      "source": [
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256, 256, 3])\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
        "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
        "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
        "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
        "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
        "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
        "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
        "    downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
        "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
        "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
        "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
        "    upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
        "  ]\n",
        "\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
        "                                         strides=2,\n",
        "                                         padding='same',\n",
        "                                         kernel_initializer=initializer,\n",
        "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
        "\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma82y8iZdFxL"
      },
      "source": [
        "Visualize the generator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TFlJnzKZH6c"
      },
      "source": [
        "generator = Generator()\n",
        "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym8P6LzUdKS_"
      },
      "source": [
        "Test the generator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us3y66e6ZL5H"
      },
      "source": [
        "gen_output = generator(inp[tf.newaxis, ...], training=False)\n",
        "plt.imshow(gen_output[0, ...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g9-ERXWdRgA"
      },
      "source": [
        "#### Define the generator loss\n",
        "\n",
        "GANs learn a loss that adapts to the data, while cGANs learn a structured loss that penalizes a possible structure that differs from the network output and the target image, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- The generator loss is a sigmoid cross-entropy loss of the generated images and an **array of ones**.\n",
        "- The pix2pix paper also mentions the L1 loss, which is a MAE (mean absolute error) between the generated image and the target image.\n",
        "- This allows the generated image to become structurally similar to the target image.\n",
        "- The formula to calculate the total generator loss is `gan_loss + LAMBDA * l1_loss`, where `LAMBDA = 100`. This value was decided by the authors of the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "249FC8rrZOS9"
      },
      "source": [
        "LAMBDA = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SXE_MXyZQ1B"
      },
      "source": [
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TSXnY7DdVmb"
      },
      "source": [
        "The training procedure for the generator is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXTXoMVnZS6b"
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  # Mean absolute error\n",
        "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "\n",
        "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "  return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsHyE4pdcHL"
      },
      "source": [
        "### Build the discriminator\n",
        "\n",
        "The discriminator in the pix2pix cGAN is a convolutional PatchGAN classifier—it tries to classify if each image _patch_ is real or not real, as described in the [pix2pix paper](https://arxiv.org/abs/1611.07004).\n",
        "\n",
        "- Each block in the discriminator is: Convolution -> Batch normalization -> Leaky ReLU.\n",
        "- The shape of the output after the last layer is `(batch_size, 30, 30, 1)`.\n",
        "- Each `30 x 30` image patch of the output classifies a `70 x 70` portion of the input image.\n",
        "- The discriminator receives 2 inputs: \n",
        "    - The input image and the target image, which it should classify as real.\n",
        "    - The input image and the generated image (the output of the generator), which it should classify as fake.\n",
        "    - Use `tf.concat([inp, tar], axis=-1)` to concatenate these 2 inputs together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzGr75iRdgCs"
      },
      "source": [
        "Let's define the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpZb_172ZWWo"
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
        "                                kernel_initializer=initializer,\n",
        "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
        "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPdq4gTtdijW"
      },
      "source": [
        "Visualize the discriminator model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n6j2ZpCZYk6"
      },
      "source": [
        "discriminator = Discriminator()\n",
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAXAD-HLdln_"
      },
      "source": [
        "Test the discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oyLA0WbZasT"
      },
      "source": [
        "disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\n",
        "plt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgpT0okJdoSd"
      },
      "source": [
        "#### Define the discriminator loss\n",
        "\n",
        "- The `discriminator_loss` function takes 2 inputs: **real images** and **generated images**.\n",
        "- `real_loss` is a sigmoid cross-entropy loss of the **real images** and an **array of ones(since these are the real images)**.\n",
        "- `generated_loss` is a sigmoid cross-entropy loss of the **generated images** and an **array of zeros (since these are the fake images)**.\n",
        "- The `total_loss` is the sum of `real_loss` and `generated_loss`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk2YcVxbZc7o"
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxWcVSY5dtGW"
      },
      "source": [
        "The training procedure for the discriminator is shown below.\n",
        "\n",
        "To learn more about the architecture and the hyperparameters you can refer to the [pix2pix paper](https://arxiv.org/abs/1611.07004)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jykm4f-8dvzI"
      },
      "source": [
        "### Define the optimizers and a checkpoint-saver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBx9QLobZgPu"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y4YhYQjZi9i"
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-mUFYf7dzRw"
      },
      "source": [
        "### Generate images\n",
        "\n",
        "Write a function to plot some images during training.\n",
        "\n",
        "- Pass images from the test set to the generator.\n",
        "- The generator will then translate the input image into the output.\n",
        "- The last step is to plot the predictions and _voila_!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN5RRgpLd55Q"
      },
      "source": [
        "Note: The `training=True` is intentional here since\n",
        "you want the batch statistics, while running the model on the test dataset. If you use `training=False`, you get the accumulated statistics learned from the training dataset (which you don't want)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c60GaMNsZll7"
      },
      "source": [
        "def generate_images(model, test_input, tar):\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  display_list = [test_input[0], tar[0], prediction[0]]\n",
        "  title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "\n",
        "  for i in range(3):\n",
        "    plt.subplot(1, 3, i+1)\n",
        "    plt.title(title[i])\n",
        "    # Getting the pixel values in the [0, 1] range to plot.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evPpTKbdd9WD"
      },
      "source": [
        "Test the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px5ZDUsqZnlX"
      },
      "source": [
        "for example_input, example_target in test_dataset.take(1):\n",
        "  generate_images(generator, example_input, example_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuituElYeBTz"
      },
      "source": [
        "### Training\n",
        "\n",
        "- For each example input generates an output.\n",
        "- The discriminator receives the `input_image` and the generated image as the first input. The second input is the `input_image` and the `target_image`.\n",
        "- Next, calculate the generator and the discriminator loss.\n",
        "- Then, calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
        "- Finally, log the losses to TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_oQhLKPZrEz"
      },
      "source": [
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ImNmhYZtmx"
      },
      "source": [
        "@tf.function\n",
        "def train_step(input_image, target, step):\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    gen_output = generator(input_image, training=True)\n",
        "\n",
        "    disc_real_output = discriminator([input_image, target], training=True)\n",
        "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
        "\n",
        "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
        "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                          generator.trainable_variables)\n",
        "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                               discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                          generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                              discriminator.trainable_variables))\n",
        "\n",
        "  with summary_writer.as_default():\n",
        "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n",
        "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n",
        "    tf.summary.scalar('disc_loss', disc_loss, step=step//1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRLUHdyPeFiV"
      },
      "source": [
        "The actual training loop. Since this tutorial can run of more than one dataset, and the datasets vary greatly in size the training loop is setup to work in steps instead of epochs.\n",
        "\n",
        "- Iterates over the number of steps.\n",
        "- Every 10 steps print a dot (`.`).\n",
        "- Every 1k steps: clear the display and run `generate_images` to show the progress.\n",
        "- Every 5k steps: save a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u4c8w5tZwN1"
      },
      "source": [
        "def fit(train_ds, test_ds, steps):\n",
        "  example_input, example_target = next(iter(test_ds.take(1)))\n",
        "  start = time.time()\n",
        "\n",
        "  for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n",
        "    if (step) % 1000 == 0:\n",
        "      display.clear_output(wait=True)\n",
        "\n",
        "      if step != 0:\n",
        "        print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n",
        "\n",
        "      start = time.time()\n",
        "\n",
        "      generate_images(generator, example_input, example_target)\n",
        "      print(f\"Step: {step//1000}k\")\n",
        "\n",
        "    train_step(input_image, target, step)\n",
        "\n",
        "    # Training step\n",
        "    if (step+1) % 10 == 0:\n",
        "      print('.', end='', flush=True)\n",
        "\n",
        "\n",
        "    # Save (checkpoint) the model every 5k steps\n",
        "    if (step + 1) % 5000 == 0:\n",
        "      checkpoint.save(file_prefix=checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TywsBAleIgL"
      },
      "source": [
        "This training loop saves logs that you can view in TensorBoard to monitor the training progress.\n",
        "\n",
        "If you work on a local machine, you would launch a separate TensorBoard process. When working in a notebook, launch the viewer before starting the training to monitor with TensorBoard.\n",
        "\n",
        "To launch the viewer paste the following into a code-cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkNcagcqZygk"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIBsHeJdeLXa"
      },
      "source": [
        "Finally, run the training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xscrFglvZ262"
      },
      "source": [
        "fit(train_dataset, test_dataset, steps=40000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ez814j3ePx2"
      },
      "source": [
        "Caution: This command does not terminate. It's designed to continuously upload the results of long-running experiments. Once your data is uploaded you need to stop it using the \"interrupt execution\" option in your notebook tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5l8JkAzZ9tM"
      },
      "source": [
        "!tensorboard dev upload --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2xfjpTWeaU1"
      },
      "source": [
        "You can view the [results of a previous run](https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw) of this notebook on [TensorBoard.dev](https://tensorboard.dev/).\n",
        "\n",
        "TensorBoard.dev is a managed experience for hosting, tracking, and sharing ML experiments with everyone.\n",
        "\n",
        "It can also included inline using an `<iframe>`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBDRvSU8aEJU"
      },
      "source": [
        "display.IFrame(\n",
        "    src=\"https://tensorboard.dev/experiment/lZ0C6FONROaUMfjYkVyJqw\",\n",
        "    width=\"100%\",\n",
        "    height=\"1000px\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRkbjI_-eet1"
      },
      "source": [
        "Interpreting the logs is more subtle when training a GAN (or a cGAN like pix2pix) compared to a simple classification or regression model. Things to look for:\n",
        "\n",
        "Check that neither the generator nor the discriminator model has \"won\". If either the gen_gan_loss or the disc_loss gets very low, it's an indicator that this model is dominating the other, and you are not successfully training the combined model.\n",
        "The value log(2) = 0.69 is a good reference point for these losses, as it indicates a perplexity of 2 - the discriminator is, on average, equally uncertain about the two options.\n",
        "For the disc_loss, a value below 0.69 means the discriminator is doing better than random on the combined set of real and generated images.\n",
        "For the gen_gan_loss, a value below 0.69 means the generator is doing better than random at fooling the discriminator.\n",
        "As training progresses, the gen_l1_loss should go down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Eauvhtkl6g5"
      },
      "source": [
        "## CycleGAN Demo (Tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjgRC1AZAviD"
      },
      "source": [
        "source: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/cyclegan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlOhyvuXAzUx"
      },
      "source": [
        "This notebook demonstrates unpaired image to image translation using conditional GAN's, as described in [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593), also known as CycleGAN. The paper proposes a method that can capture the characteristics of one image domain and figure out how these characteristics could be translated into another image domain, all in the absence of any paired training examples. \n",
        "\n",
        "This notebook assumes you are familiar with Pix2Pix, which you can learn about in the [Pix2Pix tutorial](https://www.tensorflow.org/tutorials/generative/pix2pix). The code for CycleGAN is similar, the main difference is an additional loss function, and the use of unpaired training data.\n",
        "\n",
        "CycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. \n",
        "\n",
        "This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset (which is simply a directory of images).\n",
        "\n",
        "![Output Image 1](images/horse2zebra_1.png)\n",
        "![Output Image 2](images/horse2zebra_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCHA-2HWA4JS"
      },
      "source": [
        "### Set up the input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-GkBJeW_Im1"
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSLcOxs3_Ome"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkJvs_xr_RVX"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9sQjytgBF0s"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDrFm2JbBF-i"
      },
      "source": [
        "### Input Pipeline\n",
        "\n",
        "This tutorial trains a model to translate from images of horses, to images of zebras. You can find this dataset and similar ones [here](https://www.tensorflow.org/datasets/catalog/cycle_gan). \n",
        "\n",
        "As mentioned in the [paper](https://arxiv.org/abs/1703.10593), apply random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting.\n",
        "\n",
        "This is similar to what was done in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#load_the_dataset)\n",
        "\n",
        "* In random jittering, the image is resized to `286 x 286` and then randomly cropped to `256 x 256`.\n",
        "* In random mirroring, the image is randomly flipped horizontally i.e left to right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwAyzOe__UYw"
      },
      "source": [
        "dataset, metadata = tfds.load('cycle_gan/horse2zebra',\n",
        "                              with_info=True, as_supervised=True)\n",
        "\n",
        "train_horses, train_zebras = dataset['trainA'], dataset['trainB']\n",
        "test_horses, test_zebras = dataset['testA'], dataset['testB']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqq1tcPm_W_C"
      },
      "source": [
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 1\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-WH3xmo_bVD"
      },
      "source": [
        "def random_crop(image):\n",
        "  cropped_image = tf.image.random_crop(\n",
        "      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n",
        "\n",
        "  return cropped_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEJNaq8X_c5V"
      },
      "source": [
        "# normalizing the images to [-1, 1]\n",
        "def normalize(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image / 127.5) - 1\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWyFl39e_ehu"
      },
      "source": [
        "def random_jitter(image):\n",
        "  # resizing to 286 x 286 x 3\n",
        "  image = tf.image.resize(image, [286, 286],\n",
        "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "  # randomly cropping to 256 x 256 x 3\n",
        "  image = random_crop(image)\n",
        "\n",
        "  # random mirroring\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxC2lO2F_gFC"
      },
      "source": [
        "def preprocess_image_train(image, label):\n",
        "  image = random_jitter(image)\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb-R3Rbv_hrl"
      },
      "source": [
        "def preprocess_image_test(image, label):\n",
        "  image = normalize(image)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhTS_MNV_jgv"
      },
      "source": [
        "train_horses = train_horses.cache().map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "train_zebras = train_zebras.cache().map(\n",
        "    preprocess_image_train, num_parallel_calls=AUTOTUNE).shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_horses = test_horses.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "test_zebras = test_zebras.map(\n",
        "    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n",
        "    BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-KKuQtH_lEY"
      },
      "source": [
        "sample_horse = next(iter(train_horses))\n",
        "sample_zebra = next(iter(train_zebras))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdYiYoiC_m05"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Horse')\n",
        "plt.imshow(sample_horse[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Horse with random jitter')\n",
        "plt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E45Y3u5X_og9"
      },
      "source": [
        "plt.subplot(121)\n",
        "plt.title('Zebra')\n",
        "plt.imshow(sample_zebra[0] * 0.5 + 0.5)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Zebra with random jitter')\n",
        "plt.imshow(random_jitter(sample_zebra[0]) * 0.5 + 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "678GcX4jBTF0"
      },
      "source": [
        "### Import and reuse the Pix2Pix model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNU7MbYBUhX"
      },
      "source": [
        "Import the generator and the discriminator used in [Pix2Pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) via the installed [tensorflow_examples](https://github.com/tensorflow/examples) package.\n",
        "\n",
        "The model architecture used in this tutorial is very similar to what was used in [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). Some of the differences are:\n",
        "\n",
        "* Cyclegan uses [instance normalization](https://arxiv.org/abs/1607.08022) instead of [batch normalization](https://arxiv.org/abs/1502.03167).\n",
        "* The [CycleGAN paper](https://arxiv.org/abs/1703.10593) uses a modified `resnet` based generator. This tutorial is using a modified `unet` generator for simplicity.\n",
        "\n",
        "There are 2 generators (G and F) and 2 discriminators (X and Y) being trained here. \n",
        "\n",
        "* Generator `G` learns to transform image `X` to image `Y`. $(G: X -> Y)$\n",
        "* Generator `F` learns to transform image `Y` to image `X`. $(F: Y -> X)$\n",
        "* Discriminator `D_X` learns to differentiate between image `X` and generated image `X` (`F(Y)`).\n",
        "* Discriminator `D_Y` learns to differentiate between image `Y` and generated image `Y` (`G(X)`).\n",
        "\n",
        "![Cyclegan model](images/cyclegan_model.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzQSsc9H_r9Y"
      },
      "source": [
        "OUTPUT_CHANNELS = 3\n",
        "\n",
        "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
        "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
        "\n",
        "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
        "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFa8NzbU_t1A"
      },
      "source": [
        "to_zebra = generator_g(sample_horse)\n",
        "to_horse = generator_f(sample_zebra)\n",
        "plt.figure(figsize=(8, 8))\n",
        "contrast = 8\n",
        "\n",
        "imgs = [sample_horse, to_zebra, sample_zebra, to_horse]\n",
        "title = ['Horse', 'To Zebra', 'Zebra', 'To Horse']\n",
        "\n",
        "for i in range(len(imgs)):\n",
        "  plt.subplot(2, 2, i+1)\n",
        "  plt.title(title[i])\n",
        "  if i % 2 == 0:\n",
        "    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n",
        "  else:\n",
        "    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNVUEF3k_v-A"
      },
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.title('Is a real zebra?')\n",
        "plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.title('Is a real horse?')\n",
        "plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMn3xq6IBZmU"
      },
      "source": [
        "### Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOBNAmu7BdMF"
      },
      "source": [
        "In CycleGAN, there is no paired data to train on, hence there is no guarantee that the input `x` and the target `y` pair are meaningful during training. Thus in order to enforce that the network learns the correct mapping, the authors propose the cycle consistency loss.\n",
        "\n",
        "The discriminator loss and the generator loss are similar to the ones used in [pix2pix](https://www.tensorflow.org/tutorials/generative/pix2pix#build_the_generator)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qqu_F2M_xlG"
      },
      "source": [
        "LAMBDA = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8nyLmQQ_zXM"
      },
      "source": [
        "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njhDt5W8_1J2"
      },
      "source": [
        "def discriminator_loss(real, generated):\n",
        "  real_loss = loss_obj(tf.ones_like(real), real)\n",
        "\n",
        "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss * 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9nncPfl_2jM"
      },
      "source": [
        "def generator_loss(generated):\n",
        "  return loss_obj(tf.ones_like(generated), generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHbr2tfpBkSt"
      },
      "source": [
        "Cycle consistency means the result should be close to the original input. For example, if one translates a sentence from English to French, and then translates it back from French to English, then the resulting sentence should be the same as the  original sentence.\n",
        "\n",
        "In cycle consistency loss, \n",
        "\n",
        "* Image $X$ is passed via generator $G$ that yields generated image $\\hat{Y}$.\n",
        "* Generated image $\\hat{Y}$ is passed via generator $F$ that yields cycled image $\\hat{X}$.\n",
        "* Mean absolute error is calculated between $X$ and $\\hat{X}$.\n",
        "\n",
        "$$forward\\ cycle\\ consistency\\ loss: X -> G(X) -> F(G(X)) \\sim \\hat{X}$$\n",
        "\n",
        "$$backward\\ cycle\\ consistency\\ loss: Y -> F(Y) -> G(F(Y)) \\sim \\hat{Y}$$\n",
        "\n",
        "\n",
        "![Cycle loss](images/cycle_loss.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFKVZeJ2_4hW"
      },
      "source": [
        "def calc_cycle_loss(real_image, cycled_image):\n",
        "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "  \n",
        "  return LAMBDA * loss1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju-nc1UmBpf7"
      },
      "source": [
        "As shown above, generator $G$ is responsible for translating image $X$ to image $Y$. Identity loss says that, if you fed image $Y$ to generator $G$, it should yield the real image $Y$ or something close to image $Y$.\n",
        "\n",
        "If you run the zebra-to-horse model on a horse or the horse-to-zebra model on a zebra, it should not modify the image much since the image already contains the target class.\n",
        "\n",
        "$$Identity\\ loss = |G(Y) - Y| + |F(X) - X|$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V534su0P_6R4"
      },
      "source": [
        "def identity_loss(real_image, same_image):\n",
        "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "  return LAMBDA * 0.5 * loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ8ZQhqQBuCV"
      },
      "source": [
        "Initialize the optimizers for all the generators and the discriminators."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZdsIqfP_75M"
      },
      "source": [
        "generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "generator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "\n",
        "discriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22Jg_iItBxHA"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVFLNXp_9px"
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
        "                           generator_f=generator_f,\n",
        "                           discriminator_x=discriminator_x,\n",
        "                           discriminator_y=discriminator_y,\n",
        "                           generator_g_optimizer=generator_g_optimizer,\n",
        "                           generator_f_optimizer=generator_f_optimizer,\n",
        "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
        "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlXec5IhB0lx"
      },
      "source": [
        "### Training\n",
        "\n",
        "Note: This example model is trained for fewer epochs (40) than the paper (200) to keep training time reasonable for this tutorial. Predictions may be less accurate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A16lDycU__U5"
      },
      "source": [
        "EPOCHS = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdrNxAz4ABXO"
      },
      "source": [
        "def generate_images(model, test_input):\n",
        "  prediction = model(test_input)\n",
        "    \n",
        "  plt.figure(figsize=(12, 12))\n",
        "\n",
        "  display_list = [test_input[0], prediction[0]]\n",
        "  title = ['Input Image', 'Predicted Image']\n",
        "\n",
        "  for i in range(2):\n",
        "    plt.subplot(1, 2, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azcikGrnB4F-"
      },
      "source": [
        "Even though the training loop looks complicated, it consists of four basic steps:\n",
        "\n",
        "* Get the predictions.\n",
        "* Calculate the loss.\n",
        "* Calculate the gradients using backpropagation.\n",
        "* Apply the gradients to the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fwo4wfQ4AEBy"
      },
      "source": [
        "@tf.function\n",
        "def train_step(real_x, real_y):\n",
        "  # persistent is set to True because the tape is used more than\n",
        "  # once to calculate the gradients.\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Generator G translates X -> Y\n",
        "    # Generator F translates Y -> X.\n",
        "    \n",
        "    fake_y = generator_g(real_x, training=True)\n",
        "    cycled_x = generator_f(fake_y, training=True)\n",
        "\n",
        "    fake_x = generator_f(real_y, training=True)\n",
        "    cycled_y = generator_g(fake_x, training=True)\n",
        "\n",
        "    # same_x and same_y are used for identity loss.\n",
        "    same_x = generator_f(real_x, training=True)\n",
        "    same_y = generator_g(real_y, training=True)\n",
        "\n",
        "    disc_real_x = discriminator_x(real_x, training=True)\n",
        "    disc_real_y = discriminator_y(real_y, training=True)\n",
        "\n",
        "    disc_fake_x = discriminator_x(fake_x, training=True)\n",
        "    disc_fake_y = discriminator_y(fake_y, training=True)\n",
        "\n",
        "    # calculate the loss\n",
        "    gen_g_loss = generator_loss(disc_fake_y)\n",
        "    gen_f_loss = generator_loss(disc_fake_x)\n",
        "    \n",
        "    total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
        "    \n",
        "    # Total generator loss = adversarial loss + cycle loss\n",
        "    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
        "    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
        "\n",
        "    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
        "    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
        "  \n",
        "  # Calculate the gradients for generator and discriminator\n",
        "  generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
        "                                        generator_g.trainable_variables)\n",
        "  generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
        "                                        generator_f.trainable_variables)\n",
        "  \n",
        "  discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
        "                                            discriminator_x.trainable_variables)\n",
        "  discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
        "                                            discriminator_y.trainable_variables)\n",
        "  \n",
        "  # Apply the gradients to the optimizer\n",
        "  generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
        "                                            generator_g.trainable_variables))\n",
        "\n",
        "  generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
        "                                            generator_f.trainable_variables))\n",
        "  \n",
        "  discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
        "                                                discriminator_x.trainable_variables))\n",
        "  \n",
        "  discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
        "                                                discriminator_y.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uie07_n8AGq6"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  n = 0\n",
        "  for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n",
        "    train_step(image_x, image_y)\n",
        "    if n % 10 == 0:\n",
        "      print ('.', end='')\n",
        "    n += 1\n",
        "\n",
        "  clear_output(wait=True)\n",
        "  # Using a consistent image (sample_horse) so that the progress of the model\n",
        "  # is clearly visible.\n",
        "  generate_images(generator_g, sample_horse)\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
        "                                                      time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nryP56M9B72c"
      },
      "source": [
        "### Generate using test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrb3oo_9AIP4"
      },
      "source": [
        "# Run the trained model on the test dataset\n",
        "for inp in test_horses.take(5):\n",
        "  generate_images(generator_g, inp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY8XTIX7cpvC"
      },
      "source": [
        "## SRGAN Demo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkhtUSWNeVZJ"
      },
      "source": [
        "source: https://colab.research.google.com/github/sony/nnabla-examples/blob/master/interactive-demos/esrgan.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8V74-xtddEZ"
      },
      "source": [
        "### ESR-GAN\n",
        "Input | Output\n",
        "--- | ---\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/GANs/esrgan/results/baboon.png) | ![alt text](https://github.com/sony/nnabla-examples/raw/master/GANs/esrgan/results/baboon_SR.png)\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/GANs/esrgan/results/comic.png) | ![alt text](https://github.com/sony/nnabla-examples/raw/master/GANs/esrgan/results/comic_SR.png)\n",
        "![alt text](https://github.com/sony/nnabla-examples/raw/master/GANs/esrgan/results/result.png) | ![alt text](https://github.com/sony/nnabla-examples/raw/master/GANs/esrgan/results/result_SR.png)\n",
        "\n",
        "This example interactively demonstrates [ESR-GAN](https://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf), a model for super-resolution of images.\n",
        "\n",
        "**Note:** The results may not be fully apprehensible without 4K display. Also, make sure not to use JPEG-compressed images as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u34FGjMcdhWW"
      },
      "source": [
        "### Preparation\n",
        "\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be checked from the top menu (Runtime → change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQimj2_Ncu55"
      },
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples/GANs/esrgan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh8L7cstdla_"
      },
      "source": [
        "Let's also download the pre-trained weight parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkQPNChScxkg"
      },
      "source": [
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/esrgan/esrgan_latest_g.h5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xv-GpArdogG"
      },
      "source": [
        "### Upload an image\n",
        "Run the following cell to upload an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd0dWuPrc1QL"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "img = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f6hVOIldsED"
      },
      "source": [
        "Let's rename the image for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYUIrHmYdCrK"
      },
      "source": [
        "import os\n",
        "ext = os.path.splitext(list(img.keys())[-1])[-1]\n",
        "os.rename(list(img.keys())[-1], \"input_image{}\".format(ext)) \n",
        "input_img = \"input_image\" + ext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SwbvGsodv1F"
      },
      "source": [
        "### Super Resolution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBmZHNard_sB"
      },
      "source": [
        "We are now ready to apply ESR-GAN and perfor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-jpGcO1dE9f"
      },
      "source": [
        "!python inference.py --loadmodel esrgan_latest_g.h5 --input_image $input_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSE3wB6Vd1r_"
      },
      "source": [
        "Let's compare the input image with the output image. Note again that you may need a 4K display to fully appreciate the enhancement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7JGFh4odHCp"
      },
      "source": [
        "from IPython.display import Image,display\n",
        "print('Input:')\n",
        "display(Image(input_img))\n",
        "print('Output:')\n",
        "display(Image('result.png'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr-N8-MbW9Vo"
      },
      "source": [
        "## StyleGAN Demo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuiyMfSKeHTU"
      },
      "source": [
        "Source: https://colab.research.google.com/github/sony/nnabla-examples/blob/master/interactive-demos/stylegan2.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZaoC_CAbJbe"
      },
      "source": [
        "### StyleGAN2\n",
        "\n",
        "![styleGAN2 generated image sample](https://github.com/sony/nnabla-examples/raw/master/GANs/stylegan2/images/sample.png)\n",
        "\n",
        "This example demonstrates face image generation using [StyleGAN2](https://github.com/NVlabs/stylegan2). StyleGAN2 is one of the generative models which can generate high-resolution images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r07yAnbbOFY"
      },
      "source": [
        "### Preparation\n",
        "Let's start by installing nnabla and accessing [nnabla-examples repository](https://github.com/sony/nnabla-examples). If you're running on Colab, make sure that your Runtime setting is set as GPU, which can be set up from the top menu (Runtime → change runtime type), and make sure to click **Connect** on the top right-hand side of the screen before you start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv9ggNd1XAqc"
      },
      "source": [
        "!pip install nnabla-ext-cuda100\n",
        "!git clone https://github.com/sony/nnabla-examples.git\n",
        "%cd nnabla-examples/GANs/stylegan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkWYo-c4bSGW"
      },
      "source": [
        "### Get the pretrained weights\n",
        "Now we will get the pretrained weights for styleGAN2, then import some modules and do some preparation for the latter part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-yrfrkgXHlt"
      },
      "source": [
        "!wget https://nnabla.org/pretrained-models/nnabla-examples/GANs/stylegan2/styleGAN2_G_params.h5\n",
        "from generate import *\n",
        "from IPython.display import Image, display\n",
        "ctx = get_extension_context(\"cudnn\")\n",
        "nn.set_default_context(ctx)\n",
        "\n",
        "num_layers = 18\n",
        "output_dir = 'results'\n",
        "\n",
        "nn.load_parameters(\"styleGAN2_G_params.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsIhEG0KbXqD"
      },
      "source": [
        "### StyleGAN2 input config\n",
        "\n",
        "In styleGAN2, the noise input **z** is fed to the **mapping network** to produce the latent code **w**. Then **w** is modified via **truncation trick** and finally the modified latent code **w'** is injected to the **synthesis network**.\n",
        "\n",
        "With multiple latent codes **w'** coming from the **mapping network**, **synthesis network** transforms the incoming tensor and gradually converts it to an image. \n",
        "\n",
        "This is how styleGAN2 generates photo-realistic high resolution images. \n",
        "\n",
        "In the following cell,  you will choose the random seed used for sampling the noise input **z**, the value for **truncation trick**, and another random seed used for the additional noise input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3dtV1cjYYtA"
      },
      "source": [
        "#@markdown Choose the seed for noise input **z**. (This drastically changes the result)\n",
        "latent_seed = 600  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 0.5  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown Choose the seed for stochasticity input.  (This slightly changes the result)\n",
        "noise_seed = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Number of images to generate\n",
        "batch_size = 1  #@param {type: \"slider\", min: 0, max: 20, step:1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqcf63TDbdU5"
      },
      "source": [
        "### Now let's run StyleGAN2!\n",
        "Execution the following cell will run the styleGAN2. You can see by changing the value used for **truncation trick**, you will get the different results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPgUhquWYbrh"
      },
      "source": [
        "rnd = np.random.RandomState(latent_seed)\n",
        "z = rnd.randn(batch_size, 512)\n",
        "\n",
        "nn.set_auto_forward(True) \n",
        "\n",
        "style_noise = nn.NdArray.from_numpy_array(z)\n",
        "style_noises = [style_noise for _ in range(2)] \n",
        "\n",
        "rgb_output = generate(batch_size, style_noises, noise_seed, mix_after=7, truncation_psi=truncation_psi) \n",
        "\n",
        "images = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "\n",
        "# Display all the images\n",
        "for i in range(batch_size):\n",
        "  filename = f'seed{latent_seed}_{i}.png'\n",
        "  imsave(filename, images[i], channel_first=True)\n",
        "  display(Image(filename, width=512, height=512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0UVYHaPbiGU"
      },
      "source": [
        "### Try Style Mixing\n",
        "\n",
        "![styleGAN2 generated image sample](https://github.com/sony/nnabla-examples/raw/master/GANs/stylegan2/images/style_mixing_sample.png)\n",
        "\n",
        "As described above, in styleGAN2, **synthesis network** receives latent code **w** multiple times and generates images. In the previous generation, latent code **w** which **synthesis network** receives is made from one single noise input **z**. In this case, we can say that **w** controls the *style* of the generated image.\n",
        "\n",
        "Given that, with a *different* latent code **w2**, made from another noise input **z2**, **synthesis network** can generate a completely different image. So, what if we use both **w** and **w2**...? That is, *style mixing*.\n",
        "\n",
        "To be specific, using 2 latent codes **w** and **w2**, **synthesis network** can generate the image which contains both elements (i.e. hair style, face components), present in images made from **w** (controling coarse style) and **w2** (controling fine style).\n",
        "\n",
        "In the following cell, you will choose one more random seed used for sampling another noise input **z2**. \n",
        "\n",
        "You can also choose from which layer it receives the additional latent code **w2**. It slightly changes the result, so try various patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA1V_7-eYfPm"
      },
      "source": [
        "#@title StyleGAN2 style mixing config\n",
        "#@markdown Choose seed for the primary noise input **z**. This will represent coarse style.\n",
        "latent_seed = 600  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose seed for the secondary noise input **z2**. This will represent fine style.\n",
        "latent_seed2 = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose from which layer to use the secondary latent code **w2**.\n",
        "mix_after = 7  #@param {type: \"slider\", min: 0, max: 17, step:1}\n",
        "\n",
        "#@markdown Choose seed for stochasticity input.\n",
        "noise_seed = 500  #@param {type: \"slider\", min: 0, max: 1000, step:1}\n",
        "\n",
        "#@markdown Choose the value for truncation trick.\n",
        "truncation_psi = 0.5  #@param {type: \"slider\", min: 0.0, max: 1.0, step: 0.01}\n",
        "\n",
        "#@markdown Number of images made solely from coarse style noise\n",
        "batch_size_A = 1  #@param {type: \"slider\", min: 0, max: 20, step:1}\n",
        "\n",
        "#@markdown Number of images made solely from fine style noise\n",
        "batch_size_B = 4  #@param {type: \"slider\", min: 0, max: 20, step:1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ODBetL9bm0D"
      },
      "source": [
        "### Let's run style mixing.\n",
        "\n",
        "Running this cell executes style mixing and displays a generated mixed image and images made solely from **w** / **w2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qulu5aC9YiPZ"
      },
      "source": [
        "rnd1 = np.random.RandomState(latent_seed)\n",
        "z1 = nn.NdArray.from_numpy_array(rnd1.randn(batch_size_A, 512))\n",
        "\n",
        "rnd2 = np.random.RandomState(latent_seed2)\n",
        "z2 = nn.NdArray.from_numpy_array(rnd2.randn(batch_size_B, 512))\n",
        "\n",
        "nn.set_auto_forward(True)\n",
        "\n",
        "mix_image_stacks = []\n",
        "for i in range(batch_size_A):\n",
        "  image_column = []\n",
        "  for j in range(batch_size_B):\n",
        "    style_noises = [F.reshape(z1[i], (1, 512)), F.reshape(z2[j], (1, 512))]\n",
        "    rgb_output = generate(1, style_noises, noise_seed, mix_after, truncation_psi)\n",
        "    image_column.append(convert_images_to_uint8(rgb_output, drange=[-1, 1])[0])\n",
        "  image_column = np.concatenate([image for image in image_column], axis=2)\n",
        "  mix_image_stacks.append(image_column)\n",
        "mix_image_stacks = np.concatenate([image for image in mix_image_stacks], axis=1)\n",
        "\n",
        "style_noises= [z1, z1]\n",
        "rgb_output = generate(batch_size_A, style_noises, noise_seed, mix_after, truncation_psi)\n",
        "image_A = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "image_A = np.concatenate([image for image in image_A], axis=1)\n",
        "\n",
        "style_noises = [z2, z2]\n",
        "rgb_output = generate(batch_size_B, style_noises, noise_seed, mix_after, truncation_psi)\n",
        "image_B = convert_images_to_uint8(rgb_output, drange=[-1, 1])\n",
        "image_B = np.concatenate([image for image in image_B], axis=2)\n",
        "\n",
        "top_image = 255 * np.ones(rgb_output[0].shape).astype(np.uint8)\n",
        "\n",
        "top_image = np.concatenate((top_image, image_B), axis=2)\n",
        "grid_image = np.concatenate((image_A, mix_image_stacks), axis=2)\n",
        "grid_image = np.concatenate((top_image, grid_image), axis=1)\n",
        "\n",
        "imsave(\"grid.png\", grid_image, channel_first=True)\n",
        "display(Image(\"grid.png\", width=256*(batch_size_B+1), height=256*(batch_size_A+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-4G0PvDl8ko"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svpFrkZ5O1vR"
      },
      "source": [
        "# Topic 5 Basic GAN Theory and Common Problems"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFth__DFOg5D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}